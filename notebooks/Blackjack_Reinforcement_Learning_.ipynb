{"cells":[{"cell_type":"markdown","id":"04463467-51b9-4c40-9ae4-cb1edcb50a34","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"6c79430f-1d44-4fe4-9a91-a03cb3c0fcec","metadata":{},"outputs":[],"source":["# **Black Jack  Reinforcement Learning and Monte Carlo Methods**\n"]},{"cell_type":"markdown","id":"a6b3e725-c89e-487b-a415-e144ec5a50f2","metadata":{},"outputs":[],"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"8cd89d14-9924-402f-a69b-c1d18ab7e8e0","metadata":{},"outputs":[],"source":["In this project, we will try to find the best playing strategy for Black Jack by using reinforcement learning. You will explore the basics of Reinforcement Learning and Monte Carlo Methods. You will learn about training your own agent to succeed in simple and complex games.  Discover better ways to train your agent and analyze its performance.\n"]},{"cell_type":"markdown","id":"cb3d7146-b006-401f-9a71-b35af5fb9915","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Work with an OpenAI Gym environments\n","*   Explain what Reinforcement Learning is\n","*   Explain what Monte Carlo Method is\n","*   Create an agent that uses Monte Carlo Method to play Black Jack\n","*   Train and Test the agents using the Black Jack environment\n"]},{"cell_type":"markdown","id":"551239b2-e588-4c2c-9dba-931851fae4ba","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"ffea22aa-3c39-4a62-a9a5-3dccfdf16e3a","metadata":{},"outputs":[],"source":["## __Table of Contents__\n","\n","\u003col\u003e\n","    \u003cli\u003e\u003ca href=\"#Objectives\"\u003eObjectives\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\n","        \u003ca href=\"#Setup\"\u003eSetup\u003c/a\u003e\n","        \u003col\u003e\n","            \u003cli\u003e\u003ca href=\"#Installing-Required-Libraries\"\u003eInstalling Required Libraries\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#Importing-Required-Libraries\"\u003eImporting Required Libraries\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#Defining-Helper-Functions\"\u003eDefining Helper Functions\u003c/a\u003e\u003c/li\u003e\n","        \u003c/ol\u003e\n","    \u003c/li\u003e\n","    \u003cli\u003e\n","        \u003ca href=\"#What's-Reinforcement-Learning?\"\u003eWhat's Reinforcement Learning?\u003c/a\u003e\n","        \u003col\u003e\n","            \u003cli\u003e\u003ca href=\"#Basic-Terminology\"\u003eBasic Terminology\u003c/a\u003e\u003c/li\u003e\n","            \u003cli\u003e\u003ca href=\"#Reinforcement-Learning-Process\"\u003eReinforcement Learning Process\u003c/a\u003e\u003c/li\u003e\n","        \u003c/ol\u003e\n","    \u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#BlackJack-Environment\"\u003eBlackjack Environment\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Epsilon-Greedy-Policy\"\u003eEpsilon-Greedy Policy\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Monte-Carlo-Method\"\u003eMonte Carlo Method\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Exercises\"\u003eExercises\u003c/a\u003e\n","\u003c/ol\u003e\n"]},{"cell_type":"markdown","id":"b240acc3-ebc3-4238-9a17-032a1a47a060","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"39891181-32ef-4f61-8d11-5947a6930bb6","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","id":"31fe276c-54ff-4bcc-999d-56b089712004","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","id":"6385e1e3-9741-4977-82c4-edaee0a13d48","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1 math copy\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"markdown","id":"faefd92f-f855-43b3-b3c4-3ad324920466","metadata":{},"outputs":[],"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","id":"4f79e647-3a12-4253-93e1-a87b6562bc4d","metadata":{},"outputs":[],"source":["pip install gym==0.22.0 pygame"]},{"cell_type":"markdown","id":"1bdbb9cf-8665-49bc-bc7c-1302a54a1553","metadata":{},"outputs":[],"source":["Now let's download the file with functions that will be useful.\n"]},{"cell_type":"code","id":"6f05a746-06cb-4e52-b7fa-4ade9427dc5a","metadata":{},"outputs":[],"source":["!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/blackjackutility.py"]},{"cell_type":"markdown","id":"7093b889-5b9a-4c4e-84bf-f6dcfbcd0757","metadata":{},"outputs":[],"source":["Please, it's necessary to **RESTART THE KERNEL**. \n"]},{"cell_type":"markdown","id":"ecee64b4-d2de-4707-b90b-214ffb34b928","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","_We recommend you import all required libraries in one place (here):_\n"]},{"cell_type":"code","id":"f4b1f050-613e-4246-af0b-c38bf285a45e","metadata":{},"outputs":[],"source":["import gym\nimport matplotlib.pyplot as plt\nimport math \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport copy\nimport pygame\nfrom collections import defaultdict\n\n\nfrom blackjackutility import get_total, game_result"]},{"cell_type":"markdown","id":"282ffb19-a3f1-4c9e-80d0-6f06582673f0","metadata":{},"outputs":[],"source":["### Defining Helper Functions\n","\n","We will define some functions to help us train the algorithm and visualize the game results. \n","This function is used to plot the value function:\n"]},{"cell_type":"code","id":"250fcf7c-8110-4b27-a178-0bc2e896e9d7","metadata":{},"outputs":[],"source":["def plot_value_function(V):\n    \"\"\"\n    plot the estimated value function for blackjack \n    Returns:  void plots value function \n    Args:\n    V: a dictionary of estimated values for blackjack \n    \"\"\"\n    #range of player score  \n    player=[state[0]  for state in  V.keys()]\n    max_player=max(player)\n    min_player=min(player)\n    player_range=np.arange(min_player, 22, 1)\n    #range of dealer score      \n    dealer=[state[1]  for state in  V.keys()]\n    max_dealer=max(dealer)\n    min_dealer=min(dealer)\n    dealer_range=np.arange(min_dealer, 11, 1)\n    #empty array for the value function, first access in the players score second  is the dealer, third is if  there  is an ace    \n    V_plot=np.zeros((21-min_player+1,max_dealer-min_dealer+1,2))\n    #create a mesh grid for plotting \n    X,Y = np.meshgrid(dealer_range,player_range)\n\n    #populate an array  of values for different  scores not including losing scores \n    for (player,dealer,ace),v in V.items():\n        if player\u003c=21 and dealer\u003c=21:\n            V_plot[player-min_player,dealer-min_dealer,(1*ace)]=V[(player,dealer,ace)]\n\n    #plot surface        \n    fig, ax = plt.subplots(nrows=1, ncols=2, subplot_kw={'projection': '3d'})\n    ax[0].plot_wireframe(X,Y, V_plot[:,:,0])\n    ax[0].set_title('no ace')\n    ax[0].set_xlabel('dealer')\n    ax[0].set_ylabel('player ')\n    ax[0].set_zlabel('value function')\n    ax[1].plot_wireframe(X,Y, V_plot[:,:,1])\n    ax[1].set_title('no ace')\n    ax[1].set_xlabel('dealer')\n    ax[1].set_ylabel('player ')\n    ax[1].set_zlabel('value function')\n    ax[1].set_title(' ace')\n    fig.tight_layout()\n    plt.show()\n\n    #plot top view of the surface     \n    fig, ax = plt.subplots(nrows=1, ncols=2)   \n    ax[0].imshow((V_plot[:,:,0]),extent =[1,10,21,4])\n    ax[0].set_title('no ace')\n    ax[0].set_xlabel('dealer')\n    ax[0].set_ylabel('player ')   \n    im=ax[1].imshow(V_plot[:,:,1],extent =[1,10,21,4])\n    ax[1].set_title('ace')\n    ax[1].set_xlabel('dealer')\n    fig.colorbar(im, ax=ax[1])"]},{"cell_type":"markdown","id":"1f1ec0da-62a2-410d-a0a1-5d95ea22363c","metadata":{},"outputs":[],"source":["This function will plot blackjack policy:\n"]},{"cell_type":"code","id":"d8181c7c-131f-4d2c-943c-2162ad6af945","metadata":{},"outputs":[],"source":["def plot_policy_blackjack(policy):\n    \"\"\"\n    plot the policy for blackjack \n    Returns:  void plots policy function \n    Args:\n    policy: a dictionary of estimated values for blackjack \n    \"\"\"    \n    #range of player score \n    player=[state[0]  for state in  policy.keys()]\n    max_player=max(player)\n    min_player=min(player)\n    #this vale is use in RL book \n    #min_player=12\n    player_range=np.arange(min_player, 22, 1)\n    #range of dealer score      \n    dealer=[state[1]  for state in policy.keys()]\n    max_dealer=max(dealer)\n    min_dealer=min(dealer)\n    dealer_range=np.arange(min_dealer, 11, 1)\n    #empty array for the value function, first access in the players score second  is the dealer, third is if  there  is an ace    \n    policy_plot=np.zeros((21-min_player+1,max_dealer-min_dealer+1,2))\n    #create a mesh grid for plotting \n    X,Y = np.meshgrid(dealer_range,player_range)\n    \n    \n    #populate an array  of values for different  policy not including losing states above 21 \n    for (player,dealer,ace),v in policy.items():\n        if player\u003c=21 and dealer\u003c=10 and player\u003e=min_player:\n            policy_plot[player-min_player,dealer-min_dealer,(1*ace)]=policy[(player,dealer,ace)]\n\n    \n    fig, ax = plt.subplots(nrows=1, ncols=2)   \n    ax[0].imshow((policy_plot[:,:,0]),cmap=plt.get_cmap('GnBu', 2),extent =[1,10,21,4])\n    ax[0].set_title('no ace')\n    ax[0].set_xlabel('dealer')\n    ax[0].set_ylabel('player ') \n    \n\n    ax[1].set_title('ace')\n    ax[1].set_xlabel('dealer')\n    im=ax[1].imshow(policy_plot[:,:,1],extent =[1,10,21,4],cmap=plt.get_cmap('GnBu', 2))\n    fig.colorbar(im, ax=ax[1],ticks=[0 , 1])"]},{"cell_type":"markdown","id":"05dd5d54-b9d5-41d9-8148-30762430717b","metadata":{},"outputs":[],"source":["This function calculates the average number of wins for a game of blackjack given a policy:\n"]},{"cell_type":"code","id":"52f360c1-82b9-4e20-a6e0-8e704e605cfb","metadata":{},"outputs":[],"source":["def average_wins(environment,policy=None,episodes=10):\n    \"\"\"\n    This function calculates the average number of wins for a game of blackjack given a policy.\n    If no policy is provided a random policy is selected.\n    Returns: average_wins: the average number of wins \n    std_wins: the average number of wins \n    Args:\n    environment:AI gym balckjack envorment object \n    policy:policy for blackjack if none a random  action will be selected \n    episodes: number of episodes \n    \"\"\"\n\n    win_loss=np.zeros(episodes)\n\n    for episode in range(episodes):\n        state=environment.reset()\n        done=False\n\n        while (not(done)):\n            if policy and isinstance(policy[state],np.int64):\n                 \n                action=policy[state]\n                \n            else:\n                action=environment.action_space.sample()\n\n            state,reward,done,info=environment.step(action)\n        result=game_result(environment,state,show=False)\n        if reward==1:\n            win_loss[episode]=1\n        else:\n            win_loss[episode]=0  \n\n        \n    average_wins=win_loss.mean()\n    std_win=win_loss.std()/np.sqrt(episodes)\n\n    return average_wins ,std_win"]},{"cell_type":"markdown","id":"63f1c348-df5b-4936-b5a1-827e28e26d4f","metadata":{},"outputs":[],"source":["## Reinforcement Learning Explained \n","In general **Reinforcement Learning**  is just another machine learning method, which is based on rewarding desired actions/output and punishing for the undesired ones. Reinforcement learning models, just like people, are choosing which action to make based on the expected return of each action. You must give your model some input which include the information about current situation and possible actions, then you must reward it based on the output. Reinforcement learning models learn to perform a task through repeated trial and error interactions with a changing environment, without any human intervention.\n"," \n","### Basic Terminology \n","\n","* **Agent**: is your reinforcement learning model, it's a decision maker and learner, \n","\n","* **Environment**: is a world around your agent, the agent learns and acts inside of it. The environment takes the action provided by the agent and returns the next\n","**state** and the **reward**.\n","\n","* **State**: is a complete description of the state of the environment. \n","\n","* **Action**: is the way agent interacts with the environment, the moves that your agent can make. **Aciton Space** is the set of all possible actions. \n","\n","* **Reward**: is the feedback from the environment, it can be negative or positive and impacts the agent and serves as an indication to an agent of what you want it to achieve. Rewards are generally unknown and agents learn how to correctly estimate the reward.\n","\n","* **Policy**: is a rule used by an agent to decide what actions to take, given the specific state. It works as a map from state to some action or a set of probabilities for each action in the action space. \n","\n","\n","* **Value Function**: is the function that returns the expected total reward your agent can get from following the specific policy. The agent uses this value function to make decisions and learns by updating the expected reward values of the parameters of this function. In this lab we will be using the  state-action value function, so our function $Q(s,a)$ will use a state-action pair and will return an estimated reward for taking action $a$ from state $s$. \n","\n","### Reinforcement Learning Process\n","1. Agent plays a number of games\n","2. In every game, the agent chooses an **Action** form the action space by using **Policy** and **Value Function**\n","3. **Action** impacts the environment and the **Reward** and the new **State** is returned to the agent.\n","4. The agent keeps track of what reward it received after choosing a certain action from a certain set. \n","5. The after completing the game, the agent updates the estimated reward for each state and action by using the actual rewards values received while playing the game. \n","6. The whole process repeats again.\n","\n","\n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0O9IEN/reinforcement-learning-fig1-700.jpg\" width=\"30%\" alt=\"cheques image\"\u003e\n","\n","\n","Famous RL models that play Chess, Go or Atari Games on superhuman levels, are all based on the aforementioned principles and concepts. \n","\n","At first, these definitions may look confusing but don't be scared, we will review and practice them more through out the lab \n","\n","\u003cp style='color: blue'\u003eNow that you know the basics, lets find what the aforementioned things are in the Blackjack environment.\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"5a899cac-caac-46eb-ad6a-1f8614e4fba9","metadata":{},"outputs":[],"source":["## BlackJack Environment \n","\n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/blackjack.gif\" width=\"30%\" alt=\"iris image\"\u003e\n","\n","**Blackjack** is a card game played against a dealer. At the start of a round, both player and dealer are dealt 2 cards. The player can only see one of the dealer’s cards. The goal of the game is to get the value of your cards as close to 21 as possible, without crossing 21. The value of each card is listed below.\n","\n","* 10/Jack/Queen/King → 10\n","* 2 through 9 → Same value as card\n","* Ace → 1 or 11 (Player’s choice). Note that ace is called useful when It can be counted as 11 without going bust.\n","\n","If the player has less than 21, they can choose to “hit” and receive a random card from the deck. They can also choose to “stand” and keep the cards they have. If the player exceeds 21, they go “bust” and automatically lose the round. If the player has exactly 21, they automatically win. Otherwise, the player wins if they are closer to 21 than the dealer.\n","\n","\n","Let's initialize our environment and explore it a bit.\n"]},{"cell_type":"markdown","id":"cebf9f61-444b-4d87-a498-12646578ccbe","metadata":{},"outputs":[],"source":["A few notes on **Open Ai Gym**: Open Ai Gym is a toolkit for developing and comparing reinforcement learning algorithms. This is the gym open-source library, which gives you access to a standardized set of environments. It's quite useful in our case, since we are with a pretty standard environment. It also allows you to create your own custom environment but for now we will only explore a pre-define Blackjack environment. \n"]},{"cell_type":"markdown","id":"f323898f-0b52-4a75-8aec-84c432abee15","metadata":{},"outputs":[],"source":["We create an openAI gym blackjack enviroment by calling gym method, we will use the `make` function to do so.\n"]},{"cell_type":"code","id":"094da43f-1cce-4fe7-87f3-f7038ea5024c","metadata":{},"outputs":[],"source":["environment= gym.make('Blackjack-v1')"]},{"cell_type":"markdown","id":"4d8294c5-8c37-42e7-83f0-08920671b8d8","metadata":{},"outputs":[],"source":["Now let's see what the **observation space** for our environment is. Observation space is a set of all possible states. We can view the space using the method `observation_space` :\n"]},{"cell_type":"code","id":"7ae9b945-ccdc-47fe-80aa-2280a66300bb","metadata":{},"outputs":[],"source":["environment.observation_space"]},{"cell_type":"markdown","id":"9712497d-972d-4b17-b07c-b858eb2e166d","metadata":{},"outputs":[],"source":["The observation of is a 3-tuple of: the player's (you) current sum, the dealer's one showing card (1-10 where 1 is ace), and whether or not the player holds a usable ace (0 or 1) or (`True` or `False`).\n","\n","    Tuple(the players current sum, dealer's one showing card player holds, usable ace (0 or 1) )\n","Then we can explain `Tuple(Discrete(32), Discrete(11), Discrete(2))` as\n","* The highest score you can achieve is (11,10,11), and the lowest is (1), so there are 32 states for the player's score.\n","* The dealer only shows one card, which can be anything from 1 to 11\n","* The 'usable ace' space is True/False, so the size of the space is 2.\n","\n","So there are $32 \\times 11 \\times 2 = 704$ possible states. Note that we have to reset the environment to start working with it. This function resets the game environment to what it was when you started the game. It will return the initial state $S_0$:\n"]},{"cell_type":"code","id":"17cb0fc1-592c-4bf7-af82-710fbfe48b57","metadata":{},"outputs":[],"source":["state=environment.reset()\nprint(\"s_{}={}\".format(0, environment.observation_space.sample())) "]},{"cell_type":"markdown","id":"64c7a44b-34f0-416f-970d-7d8c6b908ac7","metadata":{},"outputs":[],"source":["Let's see more possible states:\n"]},{"cell_type":"code","id":"dc78a948-f551-40e3-ada0-78bf96162bbe","metadata":{},"outputs":[],"source":["for i in range(15):\n    print(\"s_{}={}\".format(i, environment.observation_space.sample()))"]},{"cell_type":"markdown","id":"f9bc5eb0-218c-4aac-a85c-23aeb3e797bc","metadata":{},"outputs":[],"source":["Let's check the action space of this environment, think what it should be before running the code:\n"]},{"cell_type":"code","id":"a8009fcb-b2cc-4057-a4ec-1271ddaa365e","metadata":{},"outputs":[],"source":["environment.action_space.n"]},{"cell_type":"markdown","id":"e9e74f48-056c-46d1-9775-f5c4c23fe80e","metadata":{},"outputs":[],"source":["Hope you guessed correctly, the answer is $2$, we can either $hit$ or $stay$: $$ A_t =0   (\\textit{stay})\\quad A_2 =1   \\textit{(hit)}$$\n"]},{"cell_type":"code","id":"6698c2ce-bf26-4406-8736-a88e5a1985a3","metadata":{},"outputs":[],"source":["for t in range(10):\n    action=environment.action_space.sample()\n    if action:\n        print(\"Hit, A_{}={}\".format(t,action))\n    else:\n        print(\"Stay\",action)"]},{"cell_type":"markdown","id":"1a859be4-c8a9-43cf-9871-fa5bbd66e5cc","metadata":{},"outputs":[],"source":["Let's also check the player and dealers cards by using `environment.player` and `environment.dealer` function \n"]},{"cell_type":"code","id":"65266e1e-b41c-4027-a64d-3c2c7b706767","metadata":{},"outputs":[],"source":["print(environment.observation_space.sample())\nprint(environment.player)\nprint(environment.dealer)"]},{"cell_type":"markdown","id":"d35d1e0f-1b39-412b-90f6-21008e2cf22c","metadata":{},"outputs":[],"source":["We will introduce a bit more terminology, **Episode** is a set of  agent-environment interactions from initial to final state, i.e it's one game that agent the plays. In addition, our agents are operating in a discrete-time game. Each time-advancing decision is a **step** (e.x. taking some action from some state). It's easy to see that each Episode consists of a series of steps. \n","\n","Let's play through some episodes of the game, choosing random actions for each step: \n"]},{"cell_type":"code","id":"59add10c-c778-41fd-ad5d-849601e66e5d","metadata":{},"outputs":[],"source":["episodes=2\nsum_=0\nresult=0\nerror=0\n\n#setting the seed for reproduceability\n\nfor episode in range(episodes):\n    state=environment.reset()\n    done=False\n    print(\"_________________________________________\")\n    print(\"episode {}\".format(episode))\n   \n\n    print(\"state:\",state)\n    print(\"player has\", environment.player)\n    print(\"the players current sum: {},\\n  dealer's one showing card: {}, usable ace: {}\".format(state[0],state[1],state[2]))\n    print('dealer', environment.dealer)\n    while (not(done)):\n    \n        action = environment.action_space.sample()\n       \n        if action:\n            print(\"hit\")\n        else:\n            print(\"stay\")\n        \n        print(\"action:\",action)\n\n        state,reward,done,info = environment.step(action)\n    print(\"Done :\", done)\n    result=game_result(environment,state)\n    sum_+=reward\nsum_"]},{"cell_type":"markdown","id":"943678bd-6b87-4fae-ae92-88ea496d62bc","metadata":{},"outputs":[],"source":["Run the code above a few more times and observe that our games are not really lucky. Try playing yourself, may be you can do better than a random action generator:\n"]},{"cell_type":"code","id":"86dd3cbb-8fe0-4a54-a125-29c626c49a8e","metadata":{},"outputs":[],"source":["episodes = 5\nyour_return = 0\nfor episode in range(episodes):\n    state = environment.reset()\n    done = False\n    print(\"_________________________________________\")\n    print(\"episode {}\".format(episode))\n    print(\"You are. the agent\")\n\n    print(\"player has\", environment.player)\n    print(\"the players current sum: {},\\n  dealer's one showing card: {}, usable ace: {}\".format(state[0],state[1],state[2]))\n    while (not(done)):\n    \n        action=int(input(\"Press one to Hit and Zero to stay\"))\n        if action:\n            print(\"hit\")\n\n        else:\n            print(\"stay\")\n        \n        print(\"action:\",action)\n\n        state,reward,done,info=environment.step(action)\n        print(state)\n    result=game_result(environment,state)\n    your_return += reward\nprint(\"your return\",your_return )"]},{"cell_type":"markdown","id":"030d4078-da7c-4462-afc3-b82f4914d972","metadata":{},"outputs":[],"source":["Well, I assume, as long as you are not a professional gambler, your results were not much better than random model results. What will help us to win at this Casino Game, is ironically a casino name method: **Monte Carlo Method**. But before jumping there, lets define some things that will build an infrastructure for our learning process and elaborate/ implement previously defined terminology. \n"]},{"cell_type":"markdown","id":"9fd96420-4d0e-49ad-9ed7-7d17221030d0","metadata":{},"outputs":[],"source":["At first we will start by exploring what greedy policy is and does:\n"]},{"cell_type":"markdown","id":"d491e954-2d24-46ba-95f6-bf156bc18281","metadata":{},"outputs":[],"source":["### Epsilon-Greedy Policy\n","\n","If you remember, as was mentioned before, policy is just a function that defines which action our agent should take based on the current state. In our environment, a simple deterministic policy $\\pi$ for the the state $(15,10,0)$ may look like: \n","\n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/Screenshot%202022-11-23%20at%2010.36.45%20AM.png\" width=\"50%\" alt=\"iris image\"\u003e\n","\n","Now let's clarify a few things with the title. **Epsilon**, is just some constant, ($0 \\leq \\epsilon \\leq 1$), and it will define some probability. **Greedy**, is a concept in computer science where a greedy algorithm is the one that makes the locally optimal choice at each stage. In our case, greedy policy implies that it will choose an action with the biggest estimated return. \n","\n","For now assume that $Q(s,a)$ is our value function, it will return an **estimated** reward based on the given state and action and let $A$ be the action space. Then our policy can be simply defined: \n","\n","\n","$$\n","\\pi(s) =\n","\\begin{cases}\n","a = max_{a^* \\in A}Q(s,a^*)) \u0026 \\text{with probability 1-}  \\epsilon \\\\\\\\\\\\\n","\\text{some }a \\in A \u0026 \\text{with probability } \\epsilon \\\\\\\\\n","\\end{cases}\n","$$\n"]},{"cell_type":"markdown","id":"e534ae95-768f-4f13-8ac1-842cc36bbe4a","metadata":{},"outputs":[],"source":["You may ask why wouldn't we always use the best action, the action with the best estimated reward, what's the point of this epsilon constant. For it we will have to learn about 2 more concepts: \n"," \n","* **Exploration** happens when the agent takes the random action to explore more opportunities, gather more information about possible actions and the environment.\n","* **Exploitation** happens when the agent makes the best decision given current information, it uses the best estimated action to maximize the reward. \n","\n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/Screenshot%202022-11-23%20at%2010.51.11%20AM.png\" width=\"50%\" alt=\"iris image\"\u003e\n","\n","As demonstrated in the diagram above, **Epsilon** defines the trade-off between Exploration and Exploitation.  We need it because the best long-term strategy may involve short-term sacrifices and in most cases, agents must explore the environment and gather enough information to make the best overall decisions. It may save our agent from doing decisions that work instead of finding the best actions. \n","\n","Let's define a new python function that will follow the epsilon probability and return an action:\n"]},{"cell_type":"code","id":"22f717a3-0afc-4a45-a2b7-fd0c68d87a1b","metadata":{},"outputs":[],"source":["def random_action(action,epsilon=0.1,n_actions=2):\n    ''' \n    This function takes the best estimated action, eplsilon, and action space \n    and returns some action. \n    '''\n    # generate a random number from 0 to 1.\n    number = np.random.rand(1)\n    \n    # if number is smaller than 1-epsilon then return best estimated action\n    if number\u003c1-epsilon:\n        return action\n    # if number is bigger or equals to 1-epsilon then return some random action from the action space\n    else:\n        action=np.random.randint(n_actions)  \n        return action "]},{"cell_type":"markdown","id":"0d491750-9a36-457e-8c80-a1eed5c6cbf4","metadata":{},"outputs":[],"source":["Now, since we already know what **Greedy Policy**, **Episode** and **Reward** are, we are ready to learn about Monte Carlo Method + some tricks to make it better. \n","Math below is may be a bit complicated, it's ok if you don't fully understand it, try understanding the general strategy of this learning algorithm and the purpose of various model parameters.\n"]},{"cell_type":"markdown","id":"2fbddcb6-2410-401b-a1c8-9fba3afd7c3c","metadata":{},"outputs":[],"source":["## Monte Carlo Method \n","#### Let's talk about the heart of our algorithm, the Value function that we will be using and how it estimates the reward for each action given the state. \n","\n","Monte Carlo Method was invented by invented by Stanislaw Ulman in the 1940s, when trying to calculate the probability of a successful Canfield solitaire (He was sick and had nothing better to do). Ulman randomly lay the cards out and simply calculated the number of successful plays. We will apply the same approach to create our value function. The basic principle of Monte Carlo method can be summarized in 4 steps: \n","\n","1. Define the Domain of Possible inputs \n","2. Generate inputs randomly from a probability distribution over the domain\n","3. Perform a deterministic computation on the inputs\n","4. Average the results\n","\n","Before we can see it in action let's define a few things. Review that **Episode** is an agent-environment interactions from initial to final states which constists of steps in a in a discrete-time game. \n","\n","Monte Carlo reinforcement learning learns from **episodes of experience**, it functions by setting the value function equal to the empirical mean return.\n","Let's assume that we have some initialized policy $\\pi$ that our agent follows. Then let's play a game once and gain the following episode: \n","\n","$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n","\n","Now let's look at an total expected reward of taking an Action $A_t$ in the state $S_t$, where t is some **time step**. \n","\n","* At **time step** $t=0$ (the first time step), the environment (including the agent) is in some state $S_t = S_0$ (the initial state), takes an action $A_t = A_0$ (the first action in the game) and receives a reward $R_t = R_0$ and the environment (including the agent) moves to a next state $S_{0+1} = S_{1}$\n","\n","Let's define a function $G$, which will just give us the expected total discounted reward at each time step: \n","$$G(t) = R_t +\\gamma R_{t+1}+\\gamma^2 R_{t+2} + ...+ \\gamma^{k}R_{t+k}$$\n","\n","Discount factor $\\gamma \\in \\left[0,1\\right]$ is an important constant. We add the initial return $R_1$ as it is, without modifying the value, then to get the total reward we are adding $R_{t+1}$ but note that the value is multiplied by $0\\leq \\gamma \\leq 1$, so $R_{t+1}$ is only partially added to $R_1$, $R_{t+2}$ is multiplied by $\\gamma^2$, $R_{t+3}$ is multiplied by $\\gamma^3$ and so on. Gamma determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. Note that if $\\gamma=0$ then total expected return will be defined just by initial reward, so agent will only learn and care about actions that produce an immediate reward. \n","\n","Now we can define our action-value function $Q_{\\pi}(S, A)$ for some sate $S$ and action $A$ as: \n","$$Q_{\\pi}(S, A) = E_{\\pi}\\left[G(t)|S_t = S, A_t = A \\right ]$$\n","\n","So value function returns the expected value of a total discounted reward $G(t)$ for the time step $t$ at which $S_t = S$ and $A_t = A$. So, as an example, our state-value for the start state $S = 0$ should look something like:\n","\n","\n","Now, after completing a series of episodes in the game, how can we adjust the expected values, or a bigger question is how's the learning process happening in Monte Carlo Method. For it we will use the concept of **Incremental means**. \n","\n","**Incremental means**, is just an average of values that's computed incrementally. Let $x_1, x_2,..., x_n$ be the set of values, Let $\\mu_1, \\mu_2, ... , \\mu_{n-1}$ be an sequence of means, where $\\mu_1 = x_1$, $\\mu_2 = \\dfrac{x_1+x_2}{2}$ and $\\mu_3 = \\dfrac{x_1+x_2+x_3}{3}$ and so on. Let's see how the mean is defined incrementally:\n","\n","\\begin{align*} \n","\\mu_n \u0026= \\frac{1}{n}\\sum_{i = 1}^{n}x_i\\\\\\\\  \n","\u0026= \\frac{1}{n} (x_n +  \\sum_{i = 1}^{n-1}x_i)\\\\\n","\u0026= \\frac{1}{n}(x_n +  (n-1)\\mu_{n-1})\\\\\n","\u0026= \\mu_{n-1} + \\frac{1}{n}(x_n - \\mu_{n-1})\n","\\end{align*}\n"]},{"cell_type":"markdown","id":"0357e093-7ad7-4031-8e27-a94985ab9ca1","metadata":{},"outputs":[],"source":["Now we can put everything together to describe the Monte Calro Method Learning Process. Let's have an episode: \n","$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n","\n","For each (state, action) pair we will keep track of the number of times this (state, action) was visited, let's define function $N(s_t,a_t)$, then every time we visit (state, action) we will update the visits counter and then adjust the running mean:\n","\\begin{align*}\n","N(S_t, A_t)\u0026+=1\\\\\\\\\n","Q(S_t, A_t)\u0026+= \\frac{1}{N(S_t, A_t)}(G(t) - Q(S_t, A_t))\n","\\end{align*}\n","\n","Let's look for at an example episode where we update our Q function:\n"]},{"cell_type":"markdown","id":"e035a21e-5518-4619-bd89-bdca2fd0279e","metadata":{},"outputs":[],"source":["\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/Screenshot%202022-11-23%20at%2011.58.48%20AM.png\" width=\"50%\" alt=\"iris image\"\u003e\n"]},{"cell_type":"markdown","id":"31e61d08-dccc-4ccd-b4c0-3a0d3206c15e","metadata":{},"outputs":[],"source":["Now we can see how in this episode, before the game begins the number of visits of this state with this action is 0, then the game starts and the number of visits functions is updated. Return is $0$ in the beginning. The action  is made, by using the predefined policy, we are changing the state and the reward is received since we have more than a dealer. The total reward is updated and the Q function is updated by calculating the average reward of making action $0$ in this state. Since it was the first time this state action pair was visited, we are just performing $1/1$ so we get $1$.\n"]},{"cell_type":"markdown","id":"eb942850-097e-4e77-ab0d-a260a6084c80","metadata":{},"outputs":[],"source":[" So now we know how to update the action-value function and how to use it in combination with our policy to maximize the rewards, it can be summarized as: \n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0O9IEN/Screenshot%202022-11-08%20at%201.27.51%20PM.png\" width=\"50%\" alt=\"iris image\"\u003e\n","\n","Monte Carlo algorithm/method is a type of **model-free** reinforcement learning, since the agent does not use predictions of the environment response, so it's not trying to create a statistical **model** of the environment.\n","\n","Before implementing it, we will add a few more tricks and parameters to make this  method more efficient.\n"]},{"cell_type":"markdown","id":"cddb3e79-9201-4c19-af38-f99a3956971a","metadata":{},"outputs":[],"source":["## Improving Monte Carlo with First Starts:\n","This concept is an important strategic decision that you have to make every time you build an episode based learning algorithm. Note that every episode, we update our Q function, based on states and actions that were visited, some state-action pairs can be visited more than one per episode. **Every-Visit MC** is Monte Carlo algorithm that averages returns for every time state-action pair is visited in an episode, where **First-visit MC** averages returns only for first time state-action pair is visited in an episode. \n","\n","Let's check its implementation in the **First-visit MC** pseudo-code: \n","\n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/Screenshot%202022-11-23%20at%2012.33.51%20PM.png\" width=\"50%\" alt=\"iris image\"\u003e\n","Where $T$ is the last step in the episode, $T-1$ is the second last one and so on.\n","\n","Since, it's impossible to have the same state appearing more than once in Blackjack episode, we will only use the First-visit implementation. \n"]},{"cell_type":"markdown","id":"5344b372-c246-44b8-80f3-83cad96958cf","metadata":{},"outputs":[],"source":["## Improving Monte Carlo with Exploring-Starts:\n","Note that in big and complicated environments not every (state,action) pair may be visited during the learning process. One possible solution to this problem may be adding **exploring-starts** method. In the beginning of each episode we are always starting in the initial state $S$, but with exploring-starts we will choose our starting state randomly. **Exploring-starts** is specifying that episodes start in a state–action pair, and that every pair has a nonzero probability of being selected as the start.\n","\n","Let's check some pseudo code, to see how exploring-starts can be implemented in to our Monte Carlo algorithm. \n","\n","\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX06PMEN/Screenshot%202022-11-09%20at%2012.24.33%20AM.png\" width=\"50%\" alt=\"iris image\"\u003e\n","\n","As you can see the main part, the only thing that changes in the code itself, is that the initial state of each episode is chosen randomly, the rest of Monte Carlo Algorithm remains unchanged.\n","\n","Note that **Exploring-starts** is partially implemented in Black Jack environment by default, since player gets a random card combination in the beginning of each game, so what's left for us is to choose a random action, to generate a random initial state-action pair. \n","\n","\u003cp style='color: blue'\u003eNow let's implement it in our new code:\u003c/p\u003e\n"]},{"cell_type":"code","id":"1194ab20-bd9c-431c-ad52-25502e4a2052","metadata":{},"outputs":[],"source":["def monte_carlo_ES( environment,N_episodes=100000, discount_factor=1,first_visit=True, epsilon =0.1, theta=0.0001):\n    \"\"\"\n    plot the policy for blackjack \n    Returns:  \n    policy: a dictionary of estimated policy for blackjack \n    V: a dictionary of estimated values for blackjack \n    Q: a dictionary of estimated action function\n    DELTA: list of deltas for each episode \n    Args:\n    environment:AI gym balckjack envorment object \n    N_episodes:number of episodes \n    discount_factor:discount factor\n    first_visit: select first-visit MC (Ture) and every-visit MC (False)\n    epsilon: epsilon value \n    theta:stoping threshold\n    \"\"\"  \n    #a dictionary of estimated values for blackjack \n    V=defaultdict(float)\n    #a dictionary of estimated action function for blackjack\n    Q=defaultdict(float)\n    # number of visits to the action function \n    NumberVisitsValue= defaultdict(float)\n    # visits to action function\n    NumberVisits= defaultdict(float)\n    #dictionary  for policy \n    policy=defaultdict(float) \n    #number  of actions \n    number_actions=environment.action_space.n\n    #list of max difference between  value functions per  iteration \n    DELTA=[]\n\n    for i in range(N_episodes):\n        #max difference between  value functions\n        delta=0\n        #list that stores each state and reward for each episode     \n        episode=[]\n        # reset the  environment for the next episode and find first state  \n        state=environment.reset()   \n        #reward for the first state\n        reward=0.0\n        #flag for end of episodes  \n        done=False\n        #action for the first state \n        action=np.random.randint(number_actions)\n        #append firt state, reward and action\n        episode.append({'state':state , 'reward':reward,'action':action})\n        #Past states for signal visit  Monte Carlo \n        state_action=[(state,action)]\n        #enumerate for each episode \n        while not(done):\n\n                #take action and find next state, reward and check if the episode is  done (True)\n                (state, reward, done, prob) = environment.step(action)\n\n                #check if a policy for the state exists  \n                if isinstance(policy[state],np.int64):\n                    #obtain action from policy\n                    action=int(policy[state])\n                    random_action(action,epsilon,number_actions)\n                else:\n                     #if no policy for the state exists  select a random  action  \n                    action=np.random.randint(number_actions)\n                #add state reward and action to list \n                episode.append({'state':state , 'reward':reward,'action':action})\n                #add  states action this is for fist visit only \n                state_action.append((state,action))\n         #reverse list as the return is calculated from the last state\n        episode.reverse()\n        #append the state-action pairs to a list \n        state_action.reverse()\n\n\n        #determine the return\n        G=0\n\n        for t,step in enumerate(episode):\n\n                #check flag for first visit\n                G=discount_factor*G+step['reward']\n                #check flag for first visit\n                if first_visit:\n                    #check if the state has been visited before \n                    if (step['state'],step['action']) not in set(state_action[t+1:]): \n\n                        #increment counter for action \n                        NumberVisits[step['state'],step['action']]+=1\n                        #increment counter for value function \n                        NumberVisitsValue[step['state']]+=1\n                        #if the action function value  does not exist, create an array  to store them \n                        if not(isinstance(Q[step['state']],np.ndarray) ):\n                            Q[step['state']]= np.zeros((number_actions))\n\n                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n                        \n                        # record the old value of the value function \n\n                        v=V[step['state']]\n                        \n                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n                        #update the policy to select the action fuciton argment with the largest value \n                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n                        #find max difference between all value functions per  iteration \n                        delta=max(delta,abs(v-V[step['state']]))\n\n\n                else:\n                         #increment counter for action \n                        NumberVisits[step['state'],step['action']]+=1\n                        #increment counter for value function \n                        NumberVisitsValue[step['state']]+=1\n                        #if the action function value  does not exist, create an array  to store them \n                        if not(isinstance(Q[step['state']],np.ndarray) ):\n                            Q[step['state']]= np.zeros((number_actions))\n\n                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n                        v=V[step['state']]\n                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n                        ##update the policy to select the action functioon argument with the largest value \n                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n                        #find max difference between all value functions per  iteration \n                        delta=max(delta,abs(v-V[step['state']]))\n            \n        DELTA.append(delta)\n        if delta\u003ctheta:\n            break\n\n    return policy, V, Q,DELTA"]},{"cell_type":"markdown","id":"883abb57-6406-46db-9313-92d3765597b9","metadata":{},"outputs":[],"source":["A few more notes about the implementation, we are using a value function $V(s)$ for some state $s$ as an indication of what our model thinks the expected return is  when following the best action given by the Q function. So $V(s) = max(Q(s,a))$ for all $a$ in the action space. Our implementation returns the value function $V$ as a dictionary. $DELTA$ indicates the value by which the $V$ function for each state was updated, for example if $V_0(s) = 0.7$ and after the next episode $V_1(s) = 0.9$ then $DELTA$ is $0.9 - 0.7 = 0.2$.  The last parameter is $theta$, that indicated a stoping threshold, so if delta is very small, there is no reason to continue training the algorithm, since updates to the values are minimal we will stop the training process. \n","\n","Lets perform Monte Carlo training with 20000 episodes to see the results of our training.  \n"]},{"cell_type":"code","id":"3b099ff3-bbe1-476f-973e-c88081f926d9","metadata":{},"outputs":[],"source":[" policy, V, Q,DELTA= monte_carlo_ES(environment,N_episodes=20000, discount_factor=1,epsilon = 0.1,first_visit=True,theta=0)  "]},{"cell_type":"markdown","id":"643ea280-97a8-435d-8a04-695a4829bdaf","metadata":{},"outputs":[],"source":["Let's plot the delta value for each episode to see how the update rates are changing:\n"]},{"cell_type":"code","id":"f43ebc2d-5f48-4253-bc0c-f8f05a6dac7a","metadata":{},"outputs":[],"source":["plt.plot(DELTA)\nplt.xlabel(\"episodes\")\nplt.ylabel(\"delt \")\nplt.show()"]},{"cell_type":"markdown","id":"11b5fcdc-ea39-440c-955d-e40758a0697a","metadata":{},"outputs":[],"source":["As you can see the delta parameter is generally decreasing, which makes sense since our model comes closer and closer to the optimal values for the value function. Speaking of which, let's see the map of expected returns based on what player has and what the dealer has. \n"]},{"cell_type":"code","id":"9fbc7b1c-4291-400d-8adb-709854d8e0fc","metadata":{},"outputs":[],"source":["plot_value_function(V)"]},{"cell_type":"markdown","id":"43543be9-3691-4785-8866-0e0782891bfe","metadata":{},"outputs":[],"source":["We see a general trend, as the score of the player increases the value function takes on higher values so our expected return grows. Let see the average result of playing ten thousand games. Let's compare the result, first  using the random policy.\n"]},{"cell_type":"code","id":"178c0e17-16d4-4fab-baa9-9d3b9938c229","metadata":{},"outputs":[],"source":["average ,std_win=average_wins(environment,episodes=10000)\nprint(\"average number of wins\", average)"]},{"cell_type":"markdown","id":"f1e81083-580c-4cb5-8541-e7b51942d90a","metadata":{},"outputs":[],"source":["So the random policy gives us around $28\\%$ of wins, now let's see what the trained policy returns:\n"]},{"cell_type":"code","id":"800dacbd-1ac6-4688-b275-e9aa4845e789","metadata":{},"outputs":[],"source":["average ,std_win=average_wins(environment,policy,episodes=10000)\nprint(\"average wins:\",average,std_win )"]},{"cell_type":"markdown","id":"11fdebf1-3e3e-4304-a644-00a4bfd9751b","metadata":{},"outputs":[],"source":["Oh wow, more than 10% increase, that's not bad at all, lets see what happens when we use more episodes to train our model. \n"]},{"cell_type":"code","id":"2d3b5bf1-669e-47d9-8e80-4e06e3c510b5","metadata":{},"outputs":[],"source":["policy, V, Q,DELTA = monte_carlo_ES( environment,N_episodes=50000, discount_factor=1,first_visit=True,theta=0)  "]},{"cell_type":"markdown","id":"0bb30a49-a1e0-499a-a723-7699ab0f7226","metadata":{},"outputs":[],"source":["Let's see what's happening with the learning rate:\n"]},{"cell_type":"code","id":"dd9c1c5d-c65c-43f0-b48a-13eb99b84c7d","metadata":{},"outputs":[],"source":["plt.plot(DELTA)\nplt.xlabel(\"episodes\")\nplt.ylabel(\"delt \")\nplt.show()"]},{"cell_type":"markdown","id":"6d364edf-5ae0-4d26-8188-3f29835d249b","metadata":{},"outputs":[],"source":["Falling as expected, now let's check what our policy returns:\n"]},{"cell_type":"code","id":"01e61860-458e-4468-91a1-615a96e66584","metadata":{},"outputs":[],"source":["plot_policy_blackjack(policy)"]},{"cell_type":"markdown","id":"82f895c4-84ca-45d9-bf19-7e32e2422707","metadata":{},"outputs":[],"source":["It looks like the optimal policy for blackjack is: If the agent has no ace, the higher the dealer is showing, the more likely the agent is to hit, the exception is if the dealer has an ace. If the agent has an ace, the strategy is different. The agent will stick if the sum of their cards is over 11 and, for the most part, hold the player's sum is over 18. Let's plot the value function to explore the expected returns after 50 000 episodes:\n"]},{"cell_type":"code","id":"fa58ee2e-c9ba-485f-a71e-bbd512580688","metadata":{},"outputs":[],"source":["plot_value_function(V)"]},{"cell_type":"markdown","id":"b314c671-a2c1-4e9e-8d80-22902ac321e3","metadata":{},"outputs":[],"source":["And, most importantly, lets compare the accuracy of the results:\n"]},{"cell_type":"code","id":"ff6938c6-a6dc-496f-a689-ebb6e748d74e","metadata":{},"outputs":[],"source":["average ,std_win=average_wins(environment,policy,episodes=10000)\nprint(\"average wins:\",average,std_win )"]},{"cell_type":"markdown","id":"634a9173-f7fb-4b54-b68f-34d78873f591","metadata":{},"outputs":[],"source":["We see as the accuracy is now approximately 43% a 15 % improvement from a random policy:\n"]},{"cell_type":"code","id":"7d05723f-6056-4381-9463-d9f7669e27ce","metadata":{},"outputs":[],"source":["accuracy = [] \nepisodes = []\n\nfor n_episode in [1,50,100,500,1000,5000,10000,50000,100000]:\n    print(\"n_episode: \", str(n_episode))\n    policy, V, Q, DELTA = monte_carlo_ES( environment,N_episodes=n_episode, discount_factor=1,first_visit=True, theta = 0)  \n    average ,std_win = average_wins(environment,policy,episodes=10000)\n    print(\"n_episode: \", str(n_episode), \" average: \", str(average))\n    accuracy.append(average)\n    episodes.append(n_episode)"]},{"cell_type":"markdown","id":"ee2e483b-39cf-446e-9431-8b694b33a363","metadata":{},"outputs":[],"source":["We see that after 50000 episodes the improvement is negligible.\n"]},{"cell_type":"code","id":"6cb7f8d7-0f69-45c4-bc5d-c2b1e370e18a","metadata":{},"outputs":[],"source":["plt.plot(episodes,accuracy)\nplt.title(\"Number of wins vs number of episodes \")\nplt.ylabel('percentage of wins')\nplt.xlabel('number of episodes ')\nplt.show()"]},{"cell_type":"markdown","id":"4b81bd45-4488-4284-85e1-f1f68fd06094","metadata":{},"outputs":[],"source":["Let's experiment with the discount factor, to see if it changes anything: \n"]},{"cell_type":"code","id":"f0930153-a168-4466-95c6-b5a2852072c9","metadata":{},"outputs":[],"source":["accuracy=[] \ndiscounts=[]\n\nfor discount in [0,0.01,0.1,0.5,1.0]:\n    policy, V, Q, delta = monte_carlo_ES( environment,N_episodes=100000, discount_factor=discount,first_visit=True, theta=0)  \n    average ,std_win=average_wins(environment,policy,episodes=10000)\n    print(\"discount: \", str(discount), \" average: \", str(average))\n    discounts.append(discount)\n    accuracy.append(average)"]},{"cell_type":"markdown","id":"951d5f5f-eb8d-4c0d-bba2-4f41bdfac6ae","metadata":{},"outputs":[],"source":["We see as the Discount factor increases, the percentage of wins increases as well, the rate of increase begins to slow down when the discount value is 0.6.\n"]},{"cell_type":"code","id":"446abdc3-838b-4bca-9107-87ff24afdcce","metadata":{},"outputs":[],"source":["plt.plot(discounts,accuracy)\nplt.title(\"percentage of wins vs discount factor \")\nplt.ylabel('percentage of wins')\nplt.xlabel('discount factor')\nplt.show()"]},{"cell_type":"markdown","id":"bb1594bb-8e56-490f-9860-aea2849fcd65","metadata":{},"outputs":[],"source":["In Conclusion, we have created a successful algorithm, that was able to give us perhaps not a winning, but an optimal strategy for blackjack. \n"]},{"cell_type":"markdown","id":"24c13319-50f5-4e64-9d47-0a56f9e53165","metadata":{},"outputs":[],"source":["# Exercises\n","\n","\u003cp style='color:blue'\u003eHere are some exercises for you to practice what you have learned.\u003c/p\u003e\n","\n","As you have noticed, there is a good amount of parameters that can be modified when training our model. Use `monte_carlo_ES` function using every-visit, use 50000 episodes and discount factor of 1\n"]},{"cell_type":"code","id":"715e5572-3e5e-4c23-ad96-fb3d7ee26425","metadata":{},"outputs":[],"source":["#TODO: implement the code with defined parameters"]},{"cell_type":"markdown","id":"9bedffdb-ba1a-404b-bfa1-9e4f44bc7f03","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","\n","```python\n","policy, V, Q, delta = monte_carlo_ES( environment,N_episodes=50000, discount_factor=1,first_visit=False, theta=0)\n","average ,std_win=average_wins(environment,policy,episodes=10000)\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"b8fda157-422d-4293-b347-0859e6f70f60","metadata":{},"outputs":[],"source":["Determine the number of wins  after 1000 episodes. \n"]},{"cell_type":"code","id":"23ab061e-abb3-4c56-9896-49e9cc5e4726","metadata":{},"outputs":[],"source":["#TODO: implement the code to calculate the winnning rate"]},{"cell_type":"markdown","id":"a9f647a6-a4dd-49ee-a0ff-5f1b8bfc612f","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","    \u003csummary\u003eClick here for Solution\u003c/summary\u003e\n","\n","```python\n","average ,std_win=average_wins(environment,policy,episodes=10000)\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"29c5ea93-dc67-4b4d-9c52-c9236c2fa161","metadata":{},"outputs":[],"source":["# Congratulations! - You have completed the lab\n"]},{"cell_type":"markdown","id":"a0e3dfdd-444b-4f90-b4b7-fdd3ad27e383","metadata":{},"outputs":[],"source":["\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\"\u003eJoseph Santarcangelo\u003c/a\u003e\n","\n","[Artem Arutyunov](https://www.linkedin.com/in/artem-arutyunov/)\n"]},{"cell_type":"markdown","id":"43037b90-b817-46c5-ad6b-911421a85df3","metadata":{},"outputs":[],"source":["## Citations:\n","\n","* [OpenAi BlackJack](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py)\n","\n","* [Algorithms and pseudocode: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction](https://www.amazon.ca/Reinforcement-Learning-Introduction-Second-Paperback/dp/B0B95WFGV6/ref=asc_df_B0B95WFGV6/?tag=googleshopc0c-20\u0026linkCode=df0\u0026hvadid=578919340205\u0026hvpos=\u0026hvnetw=g\u0026hvrand=11011609051689383259\u0026hvpone=\u0026hvptwo=\u0026hvqmt=\u0026hvdev=c\u0026hvdvcmdl=\u0026hvlocint=\u0026hvlocphy=9000828\u0026hvtargid=pla-1728961664549\u0026psc=1)\n"]},{"cell_type":"markdown","id":"ec4383fc-3063-487c-b5cf-be20f78c21e3","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"71621796-3080-46ef-8e7e-b509b47696e8","metadata":{},"outputs":[],"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2022-11-17|0.1|Artem Arutyuno|Create Lab|\n"]},{"cell_type":"markdown","id":"289bf2ca-3d35-429a-ae7d-be3aeaf77f5d","metadata":{},"outputs":[],"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]},{"cell_type":"markdown","id":"ebfc0f23-a8f5-4561-a7a7-84cb2007bf79","metadata":{},"outputs":[],"source":["IBM has a special offer for watsonx.ai a studio for new foundation models, generative AI and machine learning. To take advantage of this offer visit [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0EP1EN1318-2023-01-01\u0026context=wx\u0026apps=data_science_experience%2Cwatson_data_platform%2Ccos)\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}